{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import timm\n",
    "import torch\n",
    "import random\n",
    "import sklearn.metrics\n",
    "\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from albumentations import Compose, Normalize, Resize\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(\"../metadata/SnakeCLEF2021_train_metadata_PROD.csv\")\n",
    "\n",
    "print(len(metadata))\n",
    "\n",
    "# Change your \"image_path\"!\n",
    "metadata[\"image_path\"] = metadata[\"image_path\"].apply(lambda x: x.replace(\"/Datasets/SnakeCLEF-2021/\", '/local/nahouby/Datasets/SnakeCLEF2021/'))\n",
    "metadata.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata = metadata\n",
    "print(len(train_metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    LOGGER.info(f'[{name}] start')\n",
    "    yield\n",
    "    LOGGER.info(f'[{name}] done in {time.time() - t0:.0f} s.')\n",
    "\n",
    "    \n",
    "def init_logger(log_file='train.log'):\n",
    "    from logging import getLogger, DEBUG, FileHandler,  Formatter,  StreamHandler\n",
    "    \n",
    "    log_format = '%(asctime)s %(levelname)s %(message)s'\n",
    "    \n",
    "    stream_handler = StreamHandler()\n",
    "    stream_handler.setLevel(DEBUG)\n",
    "    stream_handler.setFormatter(Formatter(log_format))\n",
    "    \n",
    "    file_handler = FileHandler(log_file)\n",
    "    file_handler.setFormatter(Formatter(log_format))\n",
    "    \n",
    "    logger = getLogger('Herbarium')\n",
    "    logger.setLevel(DEBUG)\n",
    "    logger.addHandler(stream_handler)\n",
    "    logger.addHandler(file_handler)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "LOG_FILE = '../logs/SnakeCLEF2021-ViT_large_patch16-384-FocalLoss-FT.log'\n",
    "LOGGER = init_logger(LOG_FILE)\n",
    "\n",
    "\n",
    "def seed_torch(seed=777):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "SEED = 777\n",
    "seed_torch(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLASSES = 772\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.df['image_path'].values[idx]\n",
    "        continent = self.df['continent'].values[idx]\n",
    "        country = self.df['country'].values[idx]\n",
    "        class_id = self.df['class_id'].values[idx]\n",
    "        image = cv2.imread(file_path)\n",
    "        try:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        except:\n",
    "            print(file_path)\n",
    "                    \n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "                \n",
    "        return image, file_path, class_id, country, continent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel(architecture_name, target_size, pretrained = False):\n",
    "    net = timm.create_model(architecture_name, pretrained=pretrained)\n",
    "    net_cfg = net.default_cfg\n",
    "    last_layer = net_cfg['classifier']\n",
    "    num_ftrs = getattr(net, last_layer).in_features\n",
    "    setattr(net, last_layer, nn.Linear(num_ftrs, target_size))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "MODEL_NAME = 'vit_large_patch16_384'\n",
    "model = getModel(MODEL_NAME, N_CLASSES, pretrained=True)\n",
    "model_mean = list(model.default_cfg['mean'])\n",
    "model_std = list(model.default_cfg['std'])\n",
    "\n",
    "model.load_state_dict(torch.load('SnakeCLEF2021-ViT_large_patch16-384-50E.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT = 384\n",
    "WIDTH = 384\n",
    "\n",
    "from albumentations import HueSaturationValue, RandomCrop, HorizontalFlip, VerticalFlip, RandomBrightnessContrast, CenterCrop, PadIfNeeded, RandomResizedCrop, ShiftScaleRotate, Blur, JpegCompression, RandomShadow\n",
    "\n",
    "def get_transforms(*, data):\n",
    "    assert data in ('train', 'valid')\n",
    "\n",
    "    if data == 'train':\n",
    "        return Compose([\n",
    "            RandomResizedCrop(WIDTH, HEIGHT, scale=(0.7, 1.0)),\n",
    "            HorizontalFlip(p=0.5),\n",
    "            VerticalFlip(p=0.5),\n",
    "            ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.25, rotate_limit=45, p=.75),\n",
    "            JpegCompression(quality_lower=50, quality_upper=100),\n",
    "            #RandomShadow(),\n",
    "            Blur(blur_limit=2),\n",
    "            RandomBrightnessContrast(p=0.3),\n",
    "            HueSaturationValue(p=0.2),\n",
    "            Normalize(\n",
    "                mean=model_mean,\n",
    "                std=model_std,\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "\n",
    "    elif data == 'valid':\n",
    "        return Compose([\n",
    "            Resize(WIDTH, HEIGHT),\n",
    "            Normalize(mean = model_mean, std = model_std),\n",
    "            ToTensorV2(),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust BATCH_SIZE and ACCUMULATION_STEPS to values that if multiplied results in 64 !!!!!1\n",
    "BATCH_SIZE = 4\n",
    "ACCUMULATION_STEPS = 64\n",
    "EPOCHS = 20\n",
    "WORKERS = 16\n",
    "\n",
    "train_dataset = CustomDataset(train_metadata, transform=get_transforms(data='train'))\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalLoss(nn.modules.loss._WeightedLoss):\n",
    "    def __init__(self, weight=None, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__(weight,reduction=reduction)\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight #weight parameter will act as the alpha parameter to balance class weights\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "\n",
    "        ce_loss = F.cross_entropy(logits, target, reduction=self.reduction, weight=self.weight)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma * ce_loss).mean()\n",
    "        return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import f1_score, accuracy_score, top_k_accuracy_score\n",
    "import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "with timer('Train model'):\n",
    "    accumulation_steps = ACCUMULATION_STEPS\n",
    "    n_epochs = EPOCHS\n",
    "    lr = 0.0001\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=int(len(train_loader) / ACCUMULATION_STEPS), epochs=EPOCHS)    \n",
    "    \n",
    "    criterion = FocalLoss()\n",
    "    best_score = 0.\n",
    "    best_loss = np.inf\n",
    "    best_f1 = 0.\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "        model.train()\n",
    "        avg_loss = 0.\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for i, (images, _, labels, _, _) in tqdm.tqdm(enumerate(train_loader)):\n",
    "\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            y_preds = model(images)\n",
    "            loss = criterion(y_preds, labels)\n",
    "\n",
    "            # Scale the loss to the mean of the accumulated batch size\n",
    "            avg_loss += loss.item() / len(train_loader) \n",
    "            loss = loss / accumulation_steps\n",
    "            loss.backward()\n",
    "            if (i - 1) % accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "                scheduler.step()\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        LOGGER.debug(f'  Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f} time: {elapsed:.0f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'SnakeCLEF2021-ViT_large_patch16-384-FT-FL-OCLR-20E.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
